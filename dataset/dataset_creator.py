"""
Dataset Creator for Chinese Office Domain Intent Recognition

This module generates synthetic training data for Chinese office domain intent classification.
It creates diverse sample queries for each intent to train the Chinese RoBERTa model effectively.

The dataset includes 8 intent categories (aligned with reference implementation):
1. CHECK_PAYSLIP - Êü•ËØ¢Â∑•ËµÑÂçïÁõ∏ÂÖ≥ÈóÆÈ¢ò
2. BOOK_MEETING_ROOM - ‰ºöËÆÆÂÆ§È¢ÑËÆ¢ËØ∑Ê±Ç
3. REQUEST_LEAVE - ËØ∑ÂÅáÁî≥ËØ∑
4. CHECK_BENEFITS - Á¶èÂà©Êü•ËØ¢
5. IT_TICKET - ITÊîØÊåÅÂ∑•Âçï
6. EXPENSE_REIMBURSE - Ë¥πÁî®Êä•ÈîÄ
7. COMPANY_LOOKUP - Êü•ÂÖ¨Âè∏Áõ∏ÂÖ≥‰ø°ÊÅØ
8. USER_LOOKUP - Êü•Áî®Êà∑Áõ∏ÂÖ≥‰ø°ÊÅØ
"""

import json
import pandas as pd
import random
import argparse
from typing import List, Dict, Tuple, Optional
from pathlib import Path


class ChineseOfficeIntentDatasetCreator:
    """
    Creates a synthetic dataset for Chinese office domain intent recognition.
    
    This class generates diverse training examples for each intent category,
    ensuring the model can learn to recognize various ways users might express
    their intentions in a Chinese office environment.
    """
    
    def __init__(self, samples_per_intent: int = 100, selected_intents: Optional[List[str]] = None):
        """
        Initialize the dataset creator.
        
        Args:
            samples_per_intent: Number of training samples to generate per intent
            selected_intents: List of specific intents to generate data for. If None, generates for all intents.
        """
        self.samples_per_intent = samples_per_intent
        # Updated to match reference implementation with Chinese labels
        self.intent_labels = [
            "CHECK_PAYSLIP",
            "BOOK_MEETING_ROOM", 
            "REQUEST_LEAVE",
            "CHECK_BENEFITS",
            "IT_TICKET",
            "EXPENSE_REIMBURSE",
            "COMPANY_LOOKUP",
            "USER_LOOKUP"
        ]
        
        # Filter to selected intents if specified
        if selected_intents is not None:
            # Validate that selected intents are valid
            invalid_intents = [intent for intent in selected_intents if intent not in self.intent_labels]
            if invalid_intents:
                raise ValueError(f"Invalid intent(s): {invalid_intents}. Valid intents are: {self.intent_labels}")
            self.intent_labels = selected_intents
        
        # Define templates and vocabulary for each intent
        self._define_intent_templates()
    
    def _define_intent_templates(self):
        """
        Define template patterns and vocabulary for generating diverse Chinese queries.
        
        Each intent has multiple templates with different phrasings and structures
        to create realistic variations of how users might express their needs in Chinese.
        """
        
        # CHECK_PAYSLIP templates and vocabulary (Êü•ËØ¢Â∑•ËµÑÂçï)
        self.payslip_templates = [
            "ÊàëÊÉ≥Êü•Áúã{period}ÁöÑÂ∑•ËµÑÂçï",
            "Â∏ÆÊàëÊü•ËØ¢{period}{type}",
            "ÊàëÈúÄË¶ÅÁúã‰∏Ä‰∏ã{period}ÁöÑ{type}",
            "{period}ÁöÑÂ∑•ËµÑÊòØÂ§öÂ∞ëÔºü",
            "Êü•ÁúãÊàëÁöÑ{period}{type}",
            "ËÉΩÂ∏ÆÊàëÊü•ËØ¢{period}ÁöÑ{type}ÂêóÔºü",
            "ÊàëË¶ÅÁúã{period}ÁöÑËñ™ËµÑÊòéÁªÜ",
            "ËØ∑ÊòæÁ§∫ÊàëÁöÑ{period}{type}",
            "{period}Âèë‰∫ÜÂ§öÂ∞ëÂ∑•ËµÑÔºü",
            "ÊàëÈúÄË¶Å{period}ÁöÑ{type}‰ø°ÊÅØ"
        ]
        
        self.payslip_vocab = {
            "period": ["Ëøô‰∏™Êúà", "‰∏ä‰∏™Êúà", "Êú¨Êúà", "‰∏äÊúà", "ÂΩìÊúà", "ËøôÊúà", "ÊúàÂ∫ï", "Êú¨Âπ¥Â∫¶", "‰ªäÂπ¥", "ÂéªÂπ¥"],
            "type": ["Â∑•ËµÑÂçï", "Ëñ™ËµÑ", "Â∑•ËµÑ", "Ëñ™Ê∞¥", "Êî∂ÂÖ•", "Ëñ™ÈÖ¨", "Â∑•ËµÑÊù°", "Ëñ™ËµÑÂçï", "Êî∂ÂÖ•ÊòéÁªÜ"]
        }
        
        # BOOK_MEETING_ROOM templates (‰ºöËÆÆÂÆ§È¢ÑËÆ¢)
        self.meeting_templates = [
            "ÊàëÊÉ≥È¢ÑËÆ¢{time}ÁöÑ{room_type}",
            "ËÉΩÂ∏ÆÊàëËÆ¢‰∏Ä‰∏™{room_type}{time}Áî®ÂêóÔºü",
            "ÊàëÈúÄË¶ÅÈ¢ÑÁ∫¶{time}ÁöÑ{room_type}",
            "{time}Êúâ{room_type}ÂèØ‰ª•È¢ÑËÆ¢ÂêóÔºü",
            "ÊÉ≥ËÆ¢{time}ÁöÑ{room_type}",
            "ËØ∑Â∏ÆÊàëÂÆâÊéí{time}ÁöÑ{room_type}",
            "ÊàëË¶ÅÈ¢ÑÂÆö{room_type}Ôºå{time}",
            "ËÉΩÈ¢ÑÁ∫¶{time}ÁöÑ{room_type}ÂêóÔºü",
            "Â∏ÆÂøôËÆ¢‰∏™{room_type}Ôºå{time}Áî®",
            "{time}ÊàëÈúÄË¶ÅÁî®{room_type}"
        ]
        
        self.meeting_vocab = {
            "room_type": ["‰ºöËÆÆÂÆ§", "Â§ß‰ºöËÆÆÂÆ§", "Â∞è‰ºöËÆÆÂÆ§", "ÂüπËÆ≠ÂÆ§", "ËÆ®ËÆ∫ÂÆ§", "ËßÜÈ¢ë‰ºöËÆÆÂÆ§", "Â§öÂ™í‰ΩìÂÆ§"],
            "time": ["ÊòéÂ§©‰∏äÂçà", "‰∏ãÂçà‰∏§ÁÇπ", "ÊòéÂ§©‰∏ãÂçà", "ÂêéÂ§©", "Âë®‰∏Ä", "Âë®‰∫î‰∏ãÂçà", "ÊòéÂ§©ÂçÅÁÇπ", "‰∏ãÂë®", "‰ªäÂ§©‰∏ãÂçà"]
        }
        
        # REQUEST_LEAVE templates (ËØ∑ÂÅáÁî≥ËØ∑)
        self.leave_templates = [
            "ÊàëÊÉ≥ËØ∑{leave_type}{time}",
            "ÊàëÈúÄË¶Å{time}ËØ∑{leave_type}",
            "ËÉΩÂ∏ÆÊàëÁî≥ËØ∑{time}ÁöÑ{leave_type}ÂêóÔºü",
            "ÊàëË¶Å{time}ËØ∑{leave_type}",
            "{time}ÊàëÊÉ≥ËØ∑{leave_type}",
            "Áî≥ËØ∑{time}ÁöÑ{leave_type}",
            "ÊàëÊÉ≥{time}‰ºë{leave_type}",
            "ÈúÄË¶ÅËØ∑{leave_type}Ôºå{time}",
            "{time}ÊàëË¶ÅËØ∑ÂÅá",
            "Â∏ÆÊàëÁî≥ËØ∑{leave_type}Ôºå{time}"
        ]
        
        self.leave_vocab = {
            "leave_type": ["ÁóÖÂÅá", "‰∫ãÂÅá", "Âπ¥ÂÅá", "Â©öÂÅá", "‰∫ßÂÅá", "Ë∞É‰ºë", "ÂÅá", "ÂÅáÊúü"],
            "time": ["ÊòéÂ§©", "‰∏ãÂë®", "ËøôÂë®‰∫î", "‰∏ã‰∏™Êúà", "Ëøô‰∏™ÊúàÂ∫ï", "ÂêéÂ§©", "Âë®‰∏ÄÂà∞Âë®‰∏â", "‰∏§Â§©", "‰∏ÄÂë®"]
        }
        
        # CHECK_BENEFITS templates (Á¶èÂà©Êü•ËØ¢)
        self.benefits_templates = [
            "ÊàëÊÉ≥‰∫ÜËß£{benefit_type}",
            "ËÉΩÂëäËØâÊàë{benefit_type}ÁöÑËØ¶ÊÉÖÂêóÔºü",
            "Êü•ËØ¢{benefit_type}‰ø°ÊÅØ",
            "ÊàëÁöÑ{benefit_type}ÊúâÂì™‰∫õÔºü",
            "ËØ∑‰ªãÁªç‰∏Ä‰∏ã{benefit_type}",
            "{benefit_type}ÊÄé‰πàÁî≥ËØ∑Ôºü",
            "ÊàëÊÉ≥Áü•ÈÅì{benefit_type}ÊîøÁ≠ñ",
            "Êü•Áúã{benefit_type}ËØ¥Êòé",
            "{benefit_type}ÁöÑÊ†áÂáÜÊòØ‰ªÄ‰πàÔºü",
            "ÂÖ¨Âè∏ÁöÑ{benefit_type}Â¶Ç‰ΩïÔºü"
        ]
        
        self.benefits_vocab = {
            "benefit_type": ["Á§æ‰øù", "ÂÖ¨ÁßØÈáë", "‰øùÈô©", "Á¶èÂà©ÂæÖÈÅá", "ÂåªÁñó‰øùÈô©", "Âπ¥ÁªàÂ•ñ", "È§êË°•", "‰∫§ÈÄöË°•Ë¥¥", "‰ΩèÊàøË°•Ë¥¥", "ÂüπËÆ≠Á¶èÂà©"]
        }
        
        # IT_TICKET templates (ITÊîØÊåÅÂ∑•Âçï)
        self.it_templates = [
            "ÊàëÁöÑ{device}Êúâ{problem}",
            "{device}{problem}‰∫ÜÔºåÈúÄË¶ÅÂ∏ÆÂä©",
            "ITÊîØÊåÅÔºö{device}{problem}",
            "ÁîµËÑëÈóÆÈ¢òÔºö{problem}",
            "ÊàëÈúÄË¶ÅITÂ∏ÆÂä©Ôºå{device}{problem}",
            "{device}Âá∫Áé∞{problem}ÔºåÊÄé‰πàÂäûÔºü",
            "ÊäÄÊúØÊîØÊåÅÔºö{problem}",
            "Â∏ÆÂøôËß£ÂÜ≥{device}ÁöÑ{problem}",
            "{device}ÊúâÊïÖÈöúÔºö{problem}",
            "ITÂ∑•ÂçïÔºö{device}{problem}"
        ]
        
        self.it_vocab = {
            "device": ["ÁîµËÑë", "Á¨îËÆ∞Êú¨", "ÊâìÂç∞Êú∫", "ÁΩëÁªú", "ÈÇÆÁÆ±", "Á≥ªÁªü", "ËΩØ‰ª∂", "ËÆæÂ§á"],
            "problem": ["Êó†Ê≥ïÂºÄÊú∫", "ÁΩëÁªúËøûÊé•‰∏ç‰∏ä", "ËøêË°åÂæàÊÖ¢", "Ê≠ªÊú∫", "Êó†Ê≥ïÊâìÂç∞", "ÁôªÂΩï‰∏ç‰∫Ü", "Â¥©Ê∫É", "Âá∫Èîô"]
        }
        
        # EXPENSE_REIMBURSE templates (Ë¥πÁî®Êä•ÈîÄ)
        self.expense_templates = [
            "ÊàëÈúÄË¶ÅÊä•ÈîÄ{expense_type}",
            "Áî≥ËØ∑{expense_type}Êä•ÈîÄ",
            "ÊàëÊÉ≥Êä•ÈîÄ{expense_type}Ë¥πÁî®",
            "{expense_type}ÊÄé‰πàÊä•ÈîÄÔºü",
            "Êä•ÈîÄÁî≥ËØ∑Ôºö{expense_type}",
            "ÊàëË¶ÅÊèê‰∫§{expense_type}ÁöÑÊä•ÈîÄÂçï",
            "ËÉΩÂ∏ÆÊàëÊä•ÈîÄ{expense_type}ÂêóÔºü",
            "{expense_type}Êä•ÈîÄÊµÅÁ®ãÊòØ‰ªÄ‰πàÔºü",
            "ÊàëÊúâ{expense_type}ÈúÄË¶ÅÊä•ÈîÄ",
            "ËØ∑Â∏ÆÊàëÂ§ÑÁêÜ{expense_type}Êä•ÈîÄ"
        ]
        
        self.expense_vocab = {
            "expense_type": ["Â∑ÆÊóÖË¥π", "‰∫§ÈÄöË¥π", "È§êË¥π", "‰ΩèÂÆøË¥π", "ÂüπËÆ≠Ë¥π", "ÂäûÂÖ¨Áî®ÂìÅ", "ÈÄöËÆØË¥π", "‰ºöËÆÆË¥π", "ÊãõÂæÖË¥π", "Âä†Áè≠È§êË¥π"]
        }
        
        # COMPANY_LOOKUP templates (Êü•ÂÖ¨Âè∏)
        self.company_templates = [
            "Â∏ÆÊàë‰ªãÁªç‰∏Ä‰∏ã{company}",
            "Â∏ÆÊàëÊü•ËØ¢‰∏Ä‰∏ã{company}",
            "ÊàëÊÉ≥‰∫ÜËß£{company}",
            "Êü•ËØ¢{company}ÁöÑ‰ø°ÊÅØ",
            "ËØ∑‰ªãÁªç{company}",
            "{company}ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ",
            "ÊàëÈúÄË¶Å{company}ÁöÑËµÑÊñô",
            "ËÉΩÂëäËØâÊàë{company}ÁöÑÊÉÖÂÜµÂêóÔºü",
            "Êü•Áúã{company}ÁöÑËØ¶ÊÉÖ",
            "{company}ÊòØ‰ªÄ‰πàÂÖ¨Âè∏Ôºü"
        ]
        
        self.company_vocab = {
            "company": ["Êï∞ÁßëÂÖ¨Âè∏", "ÈÄöÁî®ÊäÄÊúØÈõÜÂõ¢Êï∞Â≠óÊô∫ËÉΩÁßëÊäÄÊúâÈôêÂÖ¨Âè∏", "Êï∞Â≠óÊô∫ËÉΩÁßëÊäÄÊúâÈôêÂÖ¨Âè∏", "ÈõÜÂõ¢ÂÖ¨Âè∏", "ÁßëÊäÄÂÖ¨Âè∏", "ÊäÄÊúØÂÖ¨Âè∏"]
        }
        
        # USER_LOOKUP templates (Êü•Áî®Êà∑) - organized by query type
        self.user_basic_templates = [
            "Â∏ÆÊàëÊü•ËØ¢‰∏Ä‰∏ã{person}",
            "Êü•ËØ¢{person}ÁöÑ‰ø°ÊÅØ",
            "ÊàëÊÉ≥‰∫ÜËß£{person}",
            "ËØ∑Êü•Êâæ{person}",
            "Â∏ÆÊàëÊâæ{person}"
        ]
        
        self.user_clarified_templates = [
            "Â∏ÆÊàëÊü•ËØ¢‰∏Ä‰∏ã{person}Ôºå{name_clarification}",
            "Êü•ËØ¢{person}Ôºå{name_clarification}",
            "ÊàëË¶ÅÊâæ{person}Ôºå{name_clarification}"
        ]
        
        self.user_company_templates = [
            "Êü•ËØ¢{company}ÁöÑ{person}",
            "Â∏ÆÊàëÊü•‰∏Ä‰∏ã{company}ÁöÑ{person}",
            "ÊàëÊÉ≥‰∫ÜËß£{company}ÁöÑ{person}",
            "{company}Êúâ‰∏™{person}ÂêóÔºü"
        ]
        
        self.user_contact_templates = [
            "Â∏ÆÊàëÊü•‰∏Ä‰∏ã{company}{person}ÁöÑ{contact_type}",
            "Êü•ËØ¢{username}ÁöÑ{contact_type}",
            "{person}ÁöÑ{contact_type}ÊòØÂ§öÂ∞ëÔºü",
            "ÊàëÈúÄË¶Å{person}ÁöÑ{contact_type}"
        ]
        
        self.user_department_templates = [
            "Â∏ÆÊàëÊü•‰∏Ä‰∏ã{company}ÁöÑ{person}ÁõÆÂâçÂú®Âì™‰∏™{org_unit}Ôºü",
            "{person}Âú®{company}ÁöÑ{job_aspect}ÊòØ‰ªÄ‰πàÔºü",
            "Â∏ÆÊàëÊü•‰∏Ä‰∏ã{company}{department}ÁöÑ{job_title}ÊòØË∞Å",
            "Â∏ÆÊàëÊü•ËØ¢‰∏Ä‰∏ã{company}{department}ÁöÑ‰∫∫ÂëòÊúâÂì™‰∫õÔºü"
        ]
        
        self.user_attribute_templates = [
            "Êü•ËØ¢‰∏Ä‰∏ã{company}{department}ÁöÑ{gender}ÂëòÂ∑•ÈÉΩÊúâË∞Å",
            "Â∏ÆÊàëÊü•ËØ¢‰∏Ä‰∏ãÂäûÂÖ¨Âú∞ÁÇπÂú®{location}ÁöÑ{person}",
            "Â∏ÆÊàëÊü•ËØ¢‰∏Ä‰∏ã{gender}ÁöÑ{person}",
            "ÂäûÂÖ¨Âú∞ÁÇπÂú®{location}ÁöÑÂëòÂ∑•ÊúâÂì™‰∫õÔºü"
        ]
        
        self.user_reverse_templates = [
            "{phone_number}ÊòØË∞ÅÁöÑ{contact_type}Ôºü",
            "Â∏ÆÊàëÊü•‰∏ã{contact_type}Â∞æÂè∑{phone_suffix}ÁöÑÁî®Êà∑ÊòØË∞ÅÔºü",
            "Ëøô‰∏™{contact_type}{phone_number}ÊòØË∞ÅÁöÑÔºü"
        ]
        
        self.user_directory_templates = [
            "Êü•ËØ¢{company}ÁöÑÈÄöËÆØÂΩï",
            "Â∏ÆÊàëÁúã‰∏Ä‰∏ã{company}ÁöÑÂëòÂ∑•ÂêçÂçï",
            "{company}ÁöÑËÅîÁ≥ª‰∫∫ÂàóË°®",
            "ÊàëÈúÄË¶Å{company}ÁöÑ‰∫∫Âëò‰ø°ÊÅØ"
        ]
        
        self.user_vocab = {
            "person": ["Âº†‰∏â", "Â≠îÊñáÁê¶", "ÊùéÂõõ", "Áéã‰∫î", "ËµµÂÖ≠", "Èôà‰∏É", "ÂàòÂÖ´", "È©¨‰πù"],
            "username": ["zhangsan1", "liwang2", "chenliu3", "maqian4", "user123", "test001"],
            "company": ["Êï∞ÁßëÂÖ¨Âè∏", "ÈÄöÁî®ÊäÄÊúØÈõÜÂõ¢Êï∞Â≠óÊô∫ËÉΩÁßëÊäÄÊúâÈôêÂÖ¨Âè∏", "ÂÖ¨Âè∏"],
            "contact_type": ["ÊâãÊú∫Âè∑", "ÂäûÂÖ¨ÁîµËØù", "ÈÇÆÁÆ±Âè∑Á†Å", "Â∫ßÊú∫Âè∑", "ÁîµËØùÂè∑Á†Å", "ËÅîÁ≥ªÊñπÂºè"],
            "name_clarification": ["Âº†ÊòØÂºìÈïøÂº†", "Áê¶ÊòØÁéãÂ≠óÊóÅÂä†Â•áÊÄ™ÁöÑÂ•á", "ÊùéÊòØÊú®Â≠êÊùé", "ÁéãÊòØ‰∏âÊ®™‰∏ÄÁ´ñÁéã"],
            "org_unit": ["ÈÉ®Èó®", "ÁßëÂÆ§", "‰∫ã‰∏öÈÉ®", "‰∏≠ÂøÉ"],
            "job_aspect": ["ËÅå‰Ωç", "Â≤ó‰Ωç", "ËßíËâ≤", "Â∑•‰Ωú"],
            "phone_number": ["13282814679", "81168151", "13912345678", "010-88888888", "0571-12345678"],
            "phone_suffix": ["2345", "8888", "1234", "6789", "0000"],
            "department": ["ÁÆ°ÊéßÊï∞Â≠óÂåñ‰∫ã‰∏öÈÉ®", "ÁªºÂêàÂäûÂÖ¨ÂÆ§", "ÊäÄÊúØÈÉ®", "Â∏ÇÂú∫ÈÉ®", "‰∫∫‰∫ãÈÉ®", "Ë¥¢Âä°ÈÉ®"],
            "job_title": ["ÊÄªÁõë", "ÊÄªÁªèÁêÜ", "ÁªèÁêÜ", "‰∏ª‰ªª", "‰∏ìÂëò", "Âä©ÁêÜ"],
            "gender": ["Áî∑ÊÄß", "Â•≥ÊÄß"],
            "location": ["Âåó‰∫¨", "‰∏äÊµ∑", "Ê∑±Âú≥", "Êù≠Â∑û", "ÂπøÂ∑û", "ÊàêÈÉΩ"]
        }
    
    def _generate_user_lookup_samples(self) -> List[str]:
        """
        Generate training samples for USER_LOOKUP intent with multiple template groups.
        
        Returns:
            List of generated text samples
        """
        samples = []
        samples_per_group = max(1, self.samples_per_intent // 8)  # Distribute across 8 template groups
        
        # Generate samples for each template group
        template_groups = [
            (self.user_basic_templates, {"person": self.user_vocab["person"]}),
            (self.user_clarified_templates, {"person": self.user_vocab["person"], "name_clarification": self.user_vocab["name_clarification"]}),
            (self.user_company_templates, {"company": self.user_vocab["company"], "person": self.user_vocab["person"]}),
            (self.user_contact_templates, {"company": self.user_vocab["company"], "person": self.user_vocab["person"], "username": self.user_vocab["username"], "contact_type": self.user_vocab["contact_type"]}),
            (self.user_department_templates, {"company": self.user_vocab["company"], "person": self.user_vocab["person"], "org_unit": self.user_vocab["org_unit"], "job_aspect": self.user_vocab["job_aspect"], "department": self.user_vocab["department"], "job_title": self.user_vocab["job_title"]}),
            (self.user_attribute_templates, {"company": self.user_vocab["company"], "department": self.user_vocab["department"], "gender": self.user_vocab["gender"], "location": self.user_vocab["location"], "person": self.user_vocab["person"]}),
            (self.user_reverse_templates, {"phone_number": self.user_vocab["phone_number"], "contact_type": self.user_vocab["contact_type"], "phone_suffix": self.user_vocab["phone_suffix"]}),
            (self.user_directory_templates, {"company": self.user_vocab["company"]})
        ]
        
        for templates, vocab in template_groups:
            for _ in range(samples_per_group):
                # Randomly select a template
                template = random.choice(templates)
                
                # Fill in placeholders with random vocabulary
                sample = template
                for placeholder, options in vocab.items():
                    if f"{{{placeholder}}}" in sample:
                        sample = sample.replace(f"{{{placeholder}}}", random.choice(options))
                
                samples.append(sample)
        
        # Fill remaining samples if we haven't reached the target
        while len(samples) < self.samples_per_intent:
            # Pick a random template group
            templates, vocab = random.choice(template_groups)
            template = random.choice(templates)
            
            sample = template
            for placeholder, options in vocab.items():
                if f"{{{placeholder}}}" in sample:
                    sample = sample.replace(f"{{{placeholder}}}", random.choice(options))
            
            samples.append(sample)
        
        return samples[:self.samples_per_intent]
    
    def _generate_samples_for_intent(self, intent: str, templates: List[str], vocab: Dict[str, List[str]]) -> List[str]:
        """
        Generate training samples for a specific intent.
        
        Args:
            intent: The intent label
            templates: List of template strings with placeholders
            vocab: Dictionary mapping placeholders to possible values
            
        Returns:
            List of generated text samples
        """
        samples = []
        
        for _ in range(self.samples_per_intent):
            # Randomly select a template
            template = random.choice(templates)
            
            # Fill in placeholders with random vocabulary
            sample = template
            for placeholder, options in vocab.items():
                if f"{{{placeholder}}}" in sample:
                    sample = sample.replace(f"{{{placeholder}}}", random.choice(options))
            
            samples.append(sample)
        
        return samples
    
    def generate_dataset(self) -> Tuple[List[str], List[str]]:
        """
        Generate the complete dataset for all intents.
        
        Returns:
            Tuple of (texts, labels) where texts are the input queries
            and labels are the corresponding intent labels
        """
        texts = []
        labels = []
        
        # Generate samples for each intent
        for intent in self.intent_labels:
            if intent == "CHECK_PAYSLIP":
                samples = self._generate_samples_for_intent(intent, self.payslip_templates, self.payslip_vocab)
            elif intent == "BOOK_MEETING_ROOM":
                samples = self._generate_samples_for_intent(intent, self.meeting_templates, self.meeting_vocab)
            elif intent == "REQUEST_LEAVE":
                samples = self._generate_samples_for_intent(intent, self.leave_templates, self.leave_vocab)
            elif intent == "CHECK_BENEFITS":
                samples = self._generate_samples_for_intent(intent, self.benefits_templates, self.benefits_vocab)
            elif intent == "IT_TICKET":
                samples = self._generate_samples_for_intent(intent, self.it_templates, self.it_vocab)
            elif intent == "EXPENSE_REIMBURSE":
                samples = self._generate_samples_for_intent(intent, self.expense_templates, self.expense_vocab)
            elif intent == "COMPANY_LOOKUP":
                samples = self._generate_samples_for_intent(intent, self.company_templates, self.company_vocab)
            elif intent == "USER_LOOKUP":
                samples = self._generate_user_lookup_samples()
            
            # Add samples and labels
            texts.extend(samples)
            labels.extend([intent] * len(samples))
        
        return texts, labels
    
    def _load_existing_dataset(self, output_dir: str = "data") -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """
        Load existing train and test datasets if they exist.
        
        Args:
            output_dir: Directory to check for existing dataset files
            
        Returns:
            Tuple of (existing_train_df, existing_test_df) or (None, None) if not found
        """
        output_path = Path(output_dir)
        train_path = output_path / "train.csv"
        test_path = output_path / "test.csv"
        
        existing_train_df = None
        existing_test_df = None
        
        if train_path.exists():
            try:
                existing_train_df = pd.read_csv(train_path)
                print(f"‚úì Found existing training data: {len(existing_train_df)} samples")
            except Exception as e:
                print(f"‚ö† Warning: Could not load existing train.csv: {e}")
        
        if test_path.exists():
            try:
                existing_test_df = pd.read_csv(test_path)
                print(f"‚úì Found existing test data: {len(existing_test_df)} samples")
            except Exception as e:
                print(f"‚ö† Warning: Could not load existing test.csv: {e}")
        
        return existing_train_df, existing_test_df
    
    def _load_existing_label_mapping(self, output_dir: str = "data") -> Optional[Dict]:
        """
        Load existing label mapping if it exists.
        
        Args:
            output_dir: Directory to check for existing label mapping file
            
        Returns:
            Existing label mapping dictionary or None if not found
        """
        output_path = Path(output_dir)
        label_mapping_path = output_path / "label_mapping.json"
        
        if label_mapping_path.exists():
            try:
                with open(label_mapping_path, 'r') as f:
                    existing_mapping = json.load(f)
                print(f"‚úì Found existing label mapping with {len(existing_mapping.get('intent_labels', []))} labels")
                return existing_mapping
            except Exception as e:
                print(f"‚ö† Warning: Could not load existing label_mapping.json: {e}")
        
        return None
    
    def _merge_datasets(self, new_train_df: pd.DataFrame, new_test_df: pd.DataFrame, 
                       existing_train_df: pd.DataFrame, existing_test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Merge new dataset with existing dataset, removing duplicates.
        
        Args:
            new_train_df: Newly generated training data
            new_test_df: Newly generated test data
            existing_train_df: Existing training data
            existing_test_df: Existing test data
            
        Returns:
            Tuple of (merged_train_df, merged_test_df)
        """
        # Combine new and existing data
        combined_train = pd.concat([existing_train_df, new_train_df], ignore_index=True)
        combined_test = pd.concat([existing_test_df, new_test_df], ignore_index=True)
        
        # Remove exact duplicates (same text and label)
        merged_train_df = combined_train.drop_duplicates(subset=['text', 'label'], keep='first').reset_index(drop=True)
        merged_test_df = combined_test.drop_duplicates(subset=['text', 'label'], keep='first').reset_index(drop=True)
        
        print(f"üìä Merge statistics:")
        print(f"   Training: {len(existing_train_df)} existing + {len(new_train_df)} new = {len(merged_train_df)} merged (removed {len(combined_train) - len(merged_train_df)} duplicates)")
        print(f"   Test: {len(existing_test_df)} existing + {len(new_test_df)} new = {len(merged_test_df)} merged (removed {len(combined_test) - len(merged_test_df)} duplicates)")
        
        return merged_train_df, merged_test_df
    
    def create_train_test_split(self, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Create train/test split of the dataset.
        
        Args:
            test_size: Fraction of data to use for testing
            
        Returns:
            Tuple of (train_df, test_df) DataFrames
        """
        texts, labels = self.generate_dataset()
        
        # Create DataFrame
        df = pd.DataFrame({
            'text': texts,
            'label': labels
        })
        
        # Shuffle the data
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # Split by intent to ensure balanced representation
        train_dfs = []
        test_dfs = []
        
        for intent in self.intent_labels:
            intent_df = df[df['label'] == intent]
            split_idx = int(len(intent_df) * (1 - test_size))
            
            train_dfs.append(intent_df.iloc[:split_idx])
            test_dfs.append(intent_df.iloc[split_idx:])
        
        train_df = pd.concat(train_dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)
        test_df = pd.concat(test_dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)
        
        return train_df, test_df
    
    def save_dataset(self, output_dir: str = "data", merge_existing: bool = True):
        """
        Generate and save the dataset to files.
        
        Args:
            output_dir: Directory to save the dataset files
            merge_existing: Whether to merge with existing dataset files if they exist
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Generate train/test split for new data
        new_train_df, new_test_df = self.create_train_test_split()
        
        # Check for existing data and merge if requested
        if merge_existing:
            existing_train_df, existing_test_df = self._load_existing_dataset(output_dir)
            
            if existing_train_df is not None and existing_test_df is not None:
                print(f"üîÑ Merging with existing dataset...")
                train_df, test_df = self._merge_datasets(new_train_df, new_test_df, existing_train_df, existing_test_df)
            else:
                print(f"üìù No existing dataset found, creating new dataset...")
                train_df, test_df = new_train_df, new_test_df
        else:
            print(f"üìù Creating new dataset (merge_existing=False)...")
            train_df, test_df = new_train_df, new_test_df
        
        # Save as CSV files
        train_df.to_csv(output_path / "train.csv", index=False)
        test_df.to_csv(output_path / "test.csv", index=False)
        
        # Save as JSON files for easier loading
        train_data = {
            'texts': train_df['text'].tolist(),
            'labels': train_df['label'].tolist()
        }
        test_data = {
            'texts': test_df['text'].tolist(),
            'labels': test_df['label'].tolist()
        }
        
        with open(output_path / "train.json", 'w') as f:
            json.dump(train_data, f, indent=2)
        
        with open(output_path / "test.json", 'w') as f:
            json.dump(test_data, f, indent=2)
        
        # Save label mapping - handle merging with existing labels
        if merge_existing:
            existing_label_mapping = self._load_existing_label_mapping(output_dir)
            
            if existing_label_mapping is not None:
                # Get all unique labels from the final merged dataset
                all_labels = sorted(set(train_df['label'].unique().tolist() + test_df['label'].unique().tolist()))
                
                # Start with existing labels to preserve their IDs
                existing_labels = existing_label_mapping.get('intent_labels', [])
                merged_labels = existing_labels.copy()
                
                # Add any new labels that weren't in the existing mapping
                for label in all_labels:
                    if label not in merged_labels:
                        merged_labels.append(label)
                
                # Create the merged label mapping
                label_mapping = {
                    'intent_labels': merged_labels,
                    'label_to_id': {label: i for i, label in enumerate(merged_labels)},
                    'id_to_label': {i: label for i, label in enumerate(merged_labels)}
                }
                print(f"üìã Merged label mapping: {len(existing_labels)} existing + {len(merged_labels) - len(existing_labels)} new = {len(merged_labels)} total labels")
            else:
                # No existing mapping, create based on dataset labels
                all_labels = sorted(set(train_df['label'].unique().tolist() + test_df['label'].unique().tolist()))
                label_mapping = {
                    'intent_labels': all_labels,
                    'label_to_id': {label: i for i, label in enumerate(all_labels)},
                    'id_to_label': {i: label for i, label in enumerate(all_labels)}
                }
        else:
            # Not merging, use only selected intents
            label_mapping = {
                'intent_labels': self.intent_labels,
                'label_to_id': {label: i for i, label in enumerate(self.intent_labels)},
                'id_to_label': {i: label for i, label in enumerate(self.intent_labels)}
            }
        
        with open(output_path / "label_mapping.json", 'w') as f:
            json.dump(label_mapping, f, indent=2)
        
        print(f"Dataset saved to {output_path}")
        print(f"Training samples: {len(train_df)}")
        print(f"Test samples: {len(test_df)}")
        print(f"Intents: {len(label_mapping['intent_labels'])}")
        
        # Print sample distribution
        print("\nSample distribution:")
        print(train_df['label'].value_counts().sort_index())


def main():
    """Main function to create and save the Chinese dataset."""
    # Get all available intents for help text
    all_intents = [
        "CHECK_PAYSLIP",
        "BOOK_MEETING_ROOM", 
        "REQUEST_LEAVE",
        "CHECK_BENEFITS",
        "IT_TICKET",
        "EXPENSE_REIMBURSE",
        "COMPANY_LOOKUP",
        "USER_LOOKUP"
    ]
    
    parser = argparse.ArgumentParser(description="Chinese Office Domain Intent Recognition Dataset Creator")
    parser.add_argument("--intents", type=str, nargs='+', 
                       help=f"Specific intents to generate data for. Available: {all_intents}")
    parser.add_argument("--samples-per-intent", type=int, default=100,
                       help="Number of samples to generate per intent (default: 100)")
    parser.add_argument("--output-dir", type=str, default="data",
                       help="Output directory for dataset files (default: data)")
    parser.add_argument("--no-merge", action="store_true",
                       help="Don't merge with existing dataset files")
    parser.add_argument("--interactive", action="store_true",
                       help="Interactive mode to select intents")
    
    args = parser.parse_args()
    
    print("Creating Chinese Office Domain Intent Recognition Dataset...")
    print("ÂàõÂª∫‰∏≠ÊñáÂäûÂÖ¨È¢ÜÂüüÊÑèÂõæËØÜÂà´Êï∞ÊçÆÈõÜ...")
    print()
    
    selected_intents = None
    
    if args.interactive:
        # Interactive mode
        print("Available intents:")
        for i, intent in enumerate(all_intents, 1):
            print(f"  {i}. {intent}")
        print()
        
        while True:
            choice = input("Select intents (comma-separated numbers, or 'all' for all intents): ").strip()
            if choice.lower() == 'all':
                selected_intents = None
                break
            
            try:
                indices = [int(x.strip()) - 1 for x in choice.split(',')]
                selected_intents = [all_intents[i] for i in indices if 0 <= i < len(all_intents)]
                if selected_intents:
                    break
                else:
                    print("‚ùå No valid intents selected. Please try again.")
            except (ValueError, IndexError):
                print("‚ùå Invalid input. Please enter comma-separated numbers or 'all'.")
    
    elif args.intents:
        selected_intents = args.intents
        # Validate selected intents
        invalid_intents = [intent for intent in selected_intents if intent not in all_intents]
        if invalid_intents:
            print(f"‚ùå Error: Invalid intent(s): {invalid_intents}")
            print(f"Available intents: {all_intents}")
            return
    
    # Show what we're generating
    if selected_intents:
        print(f"üìã Generating data for selected intents: {selected_intents}")
    else:
        print(f"üìã Generating data for all intents: {all_intents}")
        
    print(f"üìä Samples per intent: {args.samples_per_intent}")
    print(f"üìÅ Output directory: {args.output_dir}")
    print(f"üîÑ Merge with existing: {not args.no_merge}")
    print()
    
    # Create dataset creator
    try:
        creator = ChineseOfficeIntentDatasetCreator(
            samples_per_intent=args.samples_per_intent,
            selected_intents=selected_intents
        )
    except ValueError as e:
        print(f"‚ùå Error: {e}")
        return
    
    # Save dataset
    creator.save_dataset(args.output_dir, merge_existing=not args.no_merge)
    
    print("\nÊï∞ÊçÆÈõÜÂàõÂª∫ÂÆåÊàêÔºÅDataset creation completed!")
    print("ÂàõÂª∫ÁöÑÊñá‰ª∂ Files created:")
    print(f"- {args.output_dir}/train.csv")
    print(f"- {args.output_dir}/test.csv") 
    print(f"- {args.output_dir}/train.json")
    print(f"- {args.output_dir}/test.json")
    print(f"- {args.output_dir}/label_mapping.json")


if __name__ == "__main__":
    main()