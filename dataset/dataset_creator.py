"""
Dataset Creator for Chinese Office Domain Intent Recognition

This module generates synthetic training data for Chinese office domain intent classification.
It creates diverse sample queries for each intent to train the Chinese RoBERTa model effectively.

The dataset includes 8 intent categories (aligned with reference implementation):
1. CHECK_PAYSLIP - æŸ¥è¯¢å·¥èµ„å•ç›¸å…³é—®é¢˜
2. BOOK_MEETING_ROOM - ä¼šè®®å®¤é¢„è®¢è¯·æ±‚
3. REQUEST_LEAVE - è¯·å‡ç”³è¯·
4. CHECK_BENEFITS - ç¦åˆ©æŸ¥è¯¢
5. IT_TICKET - ITæ”¯æŒå·¥å•
6. EXPENSE_REIMBURSE - è´¹ç”¨æŠ¥é”€
7. COMPANY_LOOKUP - æŸ¥å…¬å¸ç›¸å…³ä¿¡æ¯
8. USER_LOOKUP - æŸ¥ç”¨æˆ·ç›¸å…³ä¿¡æ¯
"""

import json
import pandas as pd
import random
import argparse
from typing import List, Dict, Tuple, Optional
from pathlib import Path


class ChineseOfficeIntentDatasetCreator:
    """
    Creates a synthetic dataset for Chinese office domain intent recognition.
    
    This class generates diverse training examples for each intent category,
    ensuring the model can learn to recognize various ways users might express
    their intentions in a Chinese office environment.
    """
    
    def __init__(self, samples_per_intent: int = 100, selected_intents: Optional[List[str]] = None):
        """
        Initialize the dataset creator.
        
        Args:
            samples_per_intent: Number of training samples to generate per intent
            selected_intents: List of specific intents to generate data for. If None, generates for all intents.
        """
        self.samples_per_intent = samples_per_intent
        # Updated to match reference implementation with Chinese labels
        self.intent_labels = [
            "CHECK_PAYSLIP",
            "BOOK_MEETING_ROOM", 
            "REQUEST_LEAVE",
            "CHECK_BENEFITS",
            "IT_TICKET",
            "EXPENSE_REIMBURSE",
            "COMPANY_LOOKUP",
            "USER_LOOKUP"
        ]
        
        # Filter to selected intents if specified
        if selected_intents is not None:
            # Validate that selected intents are valid
            invalid_intents = [intent for intent in selected_intents if intent not in self.intent_labels]
            if invalid_intents:
                raise ValueError(f"Invalid intent(s): {invalid_intents}. Valid intents are: {self.intent_labels}")
            self.intent_labels = selected_intents
        
        # Define templates and vocabulary for each intent
        self._define_intent_templates()
    
    def _define_intent_templates(self):
        """
        Define template patterns and vocabulary for generating diverse Chinese queries.
        
        Each intent has multiple templates with different phrasings and structures
        to create realistic variations of how users might express their needs in Chinese.
        """
        
        # CHECK_PAYSLIP templates and vocabulary (æŸ¥è¯¢å·¥èµ„å•)
        self.payslip_templates = [
            "æˆ‘æƒ³æŸ¥çœ‹{period}çš„å·¥èµ„å•",
            "å¸®æˆ‘æŸ¥è¯¢{period}{type}",
            "æˆ‘éœ€è¦çœ‹ä¸€ä¸‹{period}çš„{type}",
            "{period}çš„å·¥èµ„æ˜¯å¤šå°‘ï¼Ÿ",
            "æŸ¥çœ‹æˆ‘çš„{period}{type}",
            "èƒ½å¸®æˆ‘æŸ¥è¯¢{period}çš„{type}å—ï¼Ÿ",
            "æˆ‘è¦çœ‹{period}çš„è–ªèµ„æ˜ç»†",
            "è¯·æ˜¾ç¤ºæˆ‘çš„{period}{type}",
            "{period}å‘äº†å¤šå°‘å·¥èµ„ï¼Ÿ",
            "æˆ‘éœ€è¦{period}çš„{type}ä¿¡æ¯"
        ]
        
        self.payslip_vocab = {
            "period": ["è¿™ä¸ªæœˆ", "ä¸Šä¸ªæœˆ", "æœ¬æœˆ", "ä¸Šæœˆ", "å½“æœˆ", "è¿™æœˆ", "æœˆåº•", "æœ¬å¹´åº¦", "ä»Šå¹´", "å»å¹´"],
            "type": ["å·¥èµ„å•", "è–ªèµ„", "å·¥èµ„", "è–ªæ°´", "æ”¶å…¥", "è–ªé…¬", "å·¥èµ„æ¡", "è–ªèµ„å•", "æ”¶å…¥æ˜ç»†"]
        }
        
        # BOOK_MEETING_ROOM templates (ä¼šè®®å®¤é¢„è®¢)
        self.meeting_templates = [
            "æˆ‘æƒ³é¢„è®¢{time}çš„{room_type}",
            "èƒ½å¸®æˆ‘è®¢ä¸€ä¸ª{room_type}{time}ç”¨å—ï¼Ÿ",
            "æˆ‘éœ€è¦é¢„çº¦{time}çš„{room_type}",
            "{time}æœ‰{room_type}å¯ä»¥é¢„è®¢å—ï¼Ÿ",
            "æƒ³è®¢{time}çš„{room_type}",
            "è¯·å¸®æˆ‘å®‰æ’{time}çš„{room_type}",
            "æˆ‘è¦é¢„å®š{room_type}ï¼Œ{time}",
            "èƒ½é¢„çº¦{time}çš„{room_type}å—ï¼Ÿ",
            "å¸®å¿™è®¢ä¸ª{room_type}ï¼Œ{time}ç”¨",
            "{time}æˆ‘éœ€è¦ç”¨{room_type}"
        ]
        
        self.meeting_vocab = {
            "room_type": ["ä¼šè®®å®¤", "å¤§ä¼šè®®å®¤", "å°ä¼šè®®å®¤", "åŸ¹è®­å®¤", "è®¨è®ºå®¤", "è§†é¢‘ä¼šè®®å®¤", "å¤šåª’ä½“å®¤"],
            "time": ["æ˜å¤©ä¸Šåˆ", "ä¸‹åˆä¸¤ç‚¹", "æ˜å¤©ä¸‹åˆ", "åå¤©", "å‘¨ä¸€", "å‘¨äº”ä¸‹åˆ", "æ˜å¤©åç‚¹", "ä¸‹å‘¨", "ä»Šå¤©ä¸‹åˆ"]
        }
        
        # REQUEST_LEAVE templates (è¯·å‡ç”³è¯·)
        self.leave_templates = [
            "æˆ‘æƒ³è¯·{leave_type}{time}",
            "æˆ‘éœ€è¦{time}è¯·{leave_type}",
            "èƒ½å¸®æˆ‘ç”³è¯·{time}çš„{leave_type}å—ï¼Ÿ",
            "æˆ‘è¦{time}è¯·{leave_type}",
            "{time}æˆ‘æƒ³è¯·{leave_type}",
            "ç”³è¯·{time}çš„{leave_type}",
            "æˆ‘æƒ³{time}ä¼‘{leave_type}",
            "éœ€è¦è¯·{leave_type}ï¼Œ{time}",
            "{time}æˆ‘è¦è¯·å‡",
            "å¸®æˆ‘ç”³è¯·{leave_type}ï¼Œ{time}"
        ]
        
        self.leave_vocab = {
            "leave_type": ["ç—…å‡", "äº‹å‡", "å¹´å‡", "å©šå‡", "äº§å‡", "è°ƒä¼‘", "å‡", "å‡æœŸ"],
            "time": ["æ˜å¤©", "ä¸‹å‘¨", "è¿™å‘¨äº”", "ä¸‹ä¸ªæœˆ", "è¿™ä¸ªæœˆåº•", "åå¤©", "å‘¨ä¸€åˆ°å‘¨ä¸‰", "ä¸¤å¤©", "ä¸€å‘¨"]
        }
        
        # CHECK_BENEFITS templates (ç¦åˆ©æŸ¥è¯¢)
        self.benefits_templates = [
            "æˆ‘æƒ³äº†è§£{benefit_type}",
            "èƒ½å‘Šè¯‰æˆ‘{benefit_type}çš„è¯¦æƒ…å—ï¼Ÿ",
            "æŸ¥è¯¢{benefit_type}ä¿¡æ¯",
            "æˆ‘çš„{benefit_type}æœ‰å“ªäº›ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹{benefit_type}",
            "{benefit_type}æ€ä¹ˆç”³è¯·ï¼Ÿ",
            "æˆ‘æƒ³çŸ¥é“{benefit_type}æ”¿ç­–",
            "æŸ¥çœ‹{benefit_type}è¯´æ˜",
            "{benefit_type}çš„æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å…¬å¸çš„{benefit_type}å¦‚ä½•ï¼Ÿ"
        ]
        
        self.benefits_vocab = {
            "benefit_type": ["ç¤¾ä¿", "å…¬ç§¯é‡‘", "ä¿é™©", "ç¦åˆ©å¾…é‡", "åŒ»ç–—ä¿é™©", "å¹´ç»ˆå¥–", "é¤è¡¥", "äº¤é€šè¡¥è´´", "ä½æˆ¿è¡¥è´´", "åŸ¹è®­ç¦åˆ©"]
        }
        
        # IT_TICKET templates (ITæ”¯æŒå·¥å•)
        self.it_templates = [
            "æˆ‘çš„{device}æœ‰{problem}",
            "{device}{problem}äº†ï¼Œéœ€è¦å¸®åŠ©",
            "ITæ”¯æŒï¼š{device}{problem}",
            "ç”µè„‘é—®é¢˜ï¼š{problem}",
            "æˆ‘éœ€è¦ITå¸®åŠ©ï¼Œ{device}{problem}",
            "{device}å‡ºç°{problem}ï¼Œæ€ä¹ˆåŠï¼Ÿ",
            "æŠ€æœ¯æ”¯æŒï¼š{problem}",
            "å¸®å¿™è§£å†³{device}çš„{problem}",
            "{device}æœ‰æ•…éšœï¼š{problem}",
            "ITå·¥å•ï¼š{device}{problem}"
        ]
        
        self.it_vocab = {
            "device": ["ç”µè„‘", "ç¬”è®°æœ¬", "æ‰“å°æœº", "ç½‘ç»œ", "é‚®ç®±", "ç³»ç»Ÿ", "è½¯ä»¶", "è®¾å¤‡"],
            "problem": ["æ— æ³•å¼€æœº", "ç½‘ç»œè¿æ¥ä¸ä¸Š", "è¿è¡Œå¾ˆæ…¢", "æ­»æœº", "æ— æ³•æ‰“å°", "ç™»å½•ä¸äº†", "å´©æºƒ", "å‡ºé”™"]
        }
        
        # EXPENSE_REIMBURSE templates (è´¹ç”¨æŠ¥é”€)
        self.expense_templates = [
            "æˆ‘éœ€è¦æŠ¥é”€{expense_type}",
            "ç”³è¯·{expense_type}æŠ¥é”€",
            "æˆ‘æƒ³æŠ¥é”€{expense_type}è´¹ç”¨",
            "{expense_type}æ€ä¹ˆæŠ¥é”€ï¼Ÿ",
            "æŠ¥é”€ç”³è¯·ï¼š{expense_type}",
            "æˆ‘è¦æäº¤{expense_type}çš„æŠ¥é”€å•",
            "èƒ½å¸®æˆ‘æŠ¥é”€{expense_type}å—ï¼Ÿ",
            "{expense_type}æŠ¥é”€æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æˆ‘æœ‰{expense_type}éœ€è¦æŠ¥é”€",
            "è¯·å¸®æˆ‘å¤„ç†{expense_type}æŠ¥é”€"
        ]
        
        self.expense_vocab = {
            "expense_type": ["å·®æ—…è´¹", "äº¤é€šè´¹", "é¤è´¹", "ä½å®¿è´¹", "åŸ¹è®­è´¹", "åŠå…¬ç”¨å“", "é€šè®¯è´¹", "ä¼šè®®è´¹", "æ‹›å¾…è´¹", "åŠ ç­é¤è´¹"]
        }
        
        # COMPANY_LOOKUP templates (æŸ¥å…¬å¸)
        self.company_templates = [
            "å¸®æˆ‘ä»‹ç»ä¸€ä¸‹{company}",
            "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹{company}",
            "æˆ‘æƒ³äº†è§£{company}",
            "æŸ¥è¯¢{company}çš„ä¿¡æ¯",
            "è¯·ä»‹ç»{company}",
            "{company}çš„è¯¦ç»†ä¿¡æ¯",
            "æˆ‘éœ€è¦{company}çš„èµ„æ–™",
            "èƒ½å‘Šè¯‰æˆ‘{company}çš„æƒ…å†µå—ï¼Ÿ",
            "æŸ¥çœ‹{company}çš„è¯¦æƒ…",
            "{company}æ˜¯ä»€ä¹ˆå…¬å¸ï¼Ÿ"
        ]
        
        self.company_vocab = {
            "company": ["æ•°ç§‘å…¬å¸", "é€šç”¨æŠ€æœ¯é›†å›¢æ•°å­—æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸", "æ•°å­—æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸", "é›†å›¢å…¬å¸", "ç§‘æŠ€å…¬å¸", "æŠ€æœ¯å…¬å¸"]
        }
        
        # USER_LOOKUP templates (æŸ¥ç”¨æˆ·) - organized by query type
        self.user_basic_templates = [
            "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹{person}",
            "æŸ¥è¯¢{person}çš„ä¿¡æ¯",
            "æˆ‘æƒ³äº†è§£{person}",
            "è¯·æŸ¥æ‰¾{person}",
            "å¸®æˆ‘æ‰¾{person}"
        ]
        
        self.user_clarified_templates = [
            "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹{person}ï¼Œ{name_clarification}",
            "æŸ¥è¯¢{person}ï¼Œ{name_clarification}",
            "æˆ‘è¦æ‰¾{person}ï¼Œ{name_clarification}"
        ]
        
        self.user_company_templates = [
            "æŸ¥è¯¢{company}çš„{person}",
            "å¸®æˆ‘æŸ¥ä¸€ä¸‹{company}çš„{person}",
            "æˆ‘æƒ³äº†è§£{company}çš„{person}",
            "{company}æœ‰ä¸ª{person}å—ï¼Ÿ"
        ]
        
        self.user_contact_templates = [
            "å¸®æˆ‘æŸ¥ä¸€ä¸‹{company}{person}çš„{contact_type}",
            "æŸ¥è¯¢{username}çš„{contact_type}",
            "{person}çš„{contact_type}æ˜¯å¤šå°‘ï¼Ÿ",
            "æˆ‘éœ€è¦{person}çš„{contact_type}"
        ]
        
        self.user_department_templates = [
            "å¸®æˆ‘æŸ¥ä¸€ä¸‹{company}çš„{person}ç›®å‰åœ¨å“ªä¸ª{org_unit}ï¼Ÿ",
            "{person}åœ¨{company}çš„{job_aspect}æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¸®æˆ‘æŸ¥ä¸€ä¸‹{company}{department}çš„{job_title}æ˜¯è°",
            "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹{company}{department}çš„äººå‘˜æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        self.user_attribute_templates = [
            "æŸ¥è¯¢ä¸€ä¸‹{company}{department}çš„{gender}å‘˜å·¥éƒ½æœ‰è°",
            "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹åŠå…¬åœ°ç‚¹åœ¨{location}çš„{person}",
            "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹{gender}çš„{person}",
            "åŠå…¬åœ°ç‚¹åœ¨{location}çš„å‘˜å·¥æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        self.user_reverse_templates = [
            "{phone_number}æ˜¯è°çš„{contact_type}ï¼Ÿ",
            "å¸®æˆ‘æŸ¥ä¸‹{contact_type}å°¾å·{phone_suffix}çš„ç”¨æˆ·æ˜¯è°ï¼Ÿ",
            "è¿™ä¸ª{contact_type}{phone_number}æ˜¯è°çš„ï¼Ÿ"
        ]
        
        self.user_directory_templates = [
            "æŸ¥è¯¢{company}çš„é€šè®¯å½•",
            "å¸®æˆ‘çœ‹ä¸€ä¸‹{company}çš„å‘˜å·¥åå•",
            "{company}çš„è”ç³»äººåˆ—è¡¨",
            "æˆ‘éœ€è¦{company}çš„äººå‘˜ä¿¡æ¯"
        ]
        
        self.user_vocab = {
            "person": ["å¼ ä¸‰", "å­”æ–‡ç¦", "æå››", "ç‹äº”", "èµµå…­", "é™ˆä¸ƒ", "åˆ˜å…«", "é©¬ä¹"],
            "username": ["zhangsan1", "liwang2", "chenliu3", "maqian4", "user123", "test001"],
            "company": ["æ•°ç§‘å…¬å¸", "é€šç”¨æŠ€æœ¯é›†å›¢æ•°å­—æ™ºèƒ½ç§‘æŠ€æœ‰é™å…¬å¸", "å…¬å¸"],
            "contact_type": ["æ‰‹æœºå·", "åŠå…¬ç”µè¯", "é‚®ç®±å·ç ", "åº§æœºå·", "ç”µè¯å·ç ", "è”ç³»æ–¹å¼"],
            "name_clarification": ["å¼ æ˜¯å¼“é•¿å¼ ", "ç¦æ˜¯ç‹å­—æ—åŠ å¥‡æ€ªçš„å¥‡", "ææ˜¯æœ¨å­æ", "ç‹æ˜¯ä¸‰æ¨ªä¸€ç«–ç‹"],
            "org_unit": ["éƒ¨é—¨", "ç§‘å®¤", "äº‹ä¸šéƒ¨", "ä¸­å¿ƒ"],
            "job_aspect": ["èŒä½", "å²—ä½", "è§’è‰²", "å·¥ä½œ"],
            "phone_number": ["13282814679", "81168151", "13912345678", "010-88888888", "0571-12345678"],
            "phone_suffix": ["2345", "8888", "1234", "6789", "0000"],
            "department": ["ç®¡æ§æ•°å­—åŒ–äº‹ä¸šéƒ¨", "ç»¼åˆåŠå…¬å®¤", "æŠ€æœ¯éƒ¨", "å¸‚åœºéƒ¨", "äººäº‹éƒ¨", "è´¢åŠ¡éƒ¨"],
            "job_title": ["æ€»ç›‘", "æ€»ç»ç†", "ç»ç†", "ä¸»ä»»", "ä¸“å‘˜", "åŠ©ç†"],
            "gender": ["ç”·æ€§", "å¥³æ€§"],
            "location": ["åŒ—äº¬", "ä¸Šæµ·", "æ·±åœ³", "æ­å·", "å¹¿å·", "æˆéƒ½"]
        }
    
    def _generate_user_lookup_samples(self) -> List[str]:
        """
        Generate training samples for USER_LOOKUP intent with multiple template groups.
        
        Returns:
            List of generated text samples
        """
        samples = []
        samples_per_group = max(1, self.samples_per_intent // 8)  # Distribute across 8 template groups
        
        # Generate samples for each template group
        template_groups = [
            (self.user_basic_templates, {"person": self.user_vocab["person"]}),
            (self.user_clarified_templates, {"person": self.user_vocab["person"], "name_clarification": self.user_vocab["name_clarification"]}),
            (self.user_company_templates, {"company": self.user_vocab["company"], "person": self.user_vocab["person"]}),
            (self.user_contact_templates, {"company": self.user_vocab["company"], "person": self.user_vocab["person"], "username": self.user_vocab["username"], "contact_type": self.user_vocab["contact_type"]}),
            (self.user_department_templates, {"company": self.user_vocab["company"], "person": self.user_vocab["person"], "org_unit": self.user_vocab["org_unit"], "job_aspect": self.user_vocab["job_aspect"], "department": self.user_vocab["department"], "job_title": self.user_vocab["job_title"]}),
            (self.user_attribute_templates, {"company": self.user_vocab["company"], "department": self.user_vocab["department"], "gender": self.user_vocab["gender"], "location": self.user_vocab["location"], "person": self.user_vocab["person"]}),
            (self.user_reverse_templates, {"phone_number": self.user_vocab["phone_number"], "contact_type": self.user_vocab["contact_type"], "phone_suffix": self.user_vocab["phone_suffix"]}),
            (self.user_directory_templates, {"company": self.user_vocab["company"]})
        ]
        
        for templates, vocab in template_groups:
            for _ in range(samples_per_group):
                # Randomly select a template
                template = random.choice(templates)
                
                # Fill in placeholders with random vocabulary
                sample = template
                for placeholder, options in vocab.items():
                    if f"{{{placeholder}}}" in sample:
                        sample = sample.replace(f"{{{placeholder}}}", random.choice(options))
                
                samples.append(sample)
        
        # Fill remaining samples if we haven't reached the target
        while len(samples) < self.samples_per_intent:
            # Pick a random template group
            templates, vocab = random.choice(template_groups)
            template = random.choice(templates)
            
            sample = template
            for placeholder, options in vocab.items():
                if f"{{{placeholder}}}" in sample:
                    sample = sample.replace(f"{{{placeholder}}}", random.choice(options))
            
            samples.append(sample)
        
        return samples[:self.samples_per_intent]
    
    def _generate_samples_for_intent(self, intent: str, templates: List[str], vocab: Dict[str, List[str]]) -> List[str]:
        """
        Generate training samples for a specific intent.
        
        Args:
            intent: The intent label
            templates: List of template strings with placeholders
            vocab: Dictionary mapping placeholders to possible values
            
        Returns:
            List of generated text samples
        """
        samples = []
        
        for _ in range(self.samples_per_intent):
            # Randomly select a template
            template = random.choice(templates)
            
            # Fill in placeholders with random vocabulary
            sample = template
            for placeholder, options in vocab.items():
                if f"{{{placeholder}}}" in sample:
                    sample = sample.replace(f"{{{placeholder}}}", random.choice(options))
            
            samples.append(sample)
        
        return samples
    
    def generate_dataset(self) -> Tuple[List[str], List[str]]:
        """
        Generate the complete dataset for all intents.
        
        Returns:
            Tuple of (texts, labels) where texts are the input queries
            and labels are the corresponding intent labels
        """
        texts = []
        labels = []
        
        # Generate samples for each intent
        for intent in self.intent_labels:
            if intent == "CHECK_PAYSLIP":
                samples = self._generate_samples_for_intent(intent, self.payslip_templates, self.payslip_vocab)
            elif intent == "BOOK_MEETING_ROOM":
                samples = self._generate_samples_for_intent(intent, self.meeting_templates, self.meeting_vocab)
            elif intent == "REQUEST_LEAVE":
                samples = self._generate_samples_for_intent(intent, self.leave_templates, self.leave_vocab)
            elif intent == "CHECK_BENEFITS":
                samples = self._generate_samples_for_intent(intent, self.benefits_templates, self.benefits_vocab)
            elif intent == "IT_TICKET":
                samples = self._generate_samples_for_intent(intent, self.it_templates, self.it_vocab)
            elif intent == "EXPENSE_REIMBURSE":
                samples = self._generate_samples_for_intent(intent, self.expense_templates, self.expense_vocab)
            elif intent == "COMPANY_LOOKUP":
                samples = self._generate_samples_for_intent(intent, self.company_templates, self.company_vocab)
            elif intent == "USER_LOOKUP":
                samples = self._generate_user_lookup_samples()
            
            # Add samples and labels
            texts.extend(samples)
            labels.extend([intent] * len(samples))
        
        return texts, labels
    
    def _load_existing_dataset(self, output_dir: str = "data") -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """
        Load existing train and test datasets if they exist.
        
        Args:
            output_dir: Directory to check for existing dataset files
            
        Returns:
            Tuple of (existing_train_df, existing_test_df) or (None, None) if not found
        """
        output_path = Path(output_dir)
        train_path = output_path / "train.csv"
        test_path = output_path / "test.csv"
        
        existing_train_df = None
        existing_test_df = None
        
        if train_path.exists():
            try:
                existing_train_df = pd.read_csv(train_path)
                print(f"âœ“ Found existing training data: {len(existing_train_df)} samples")
            except Exception as e:
                print(f"âš  Warning: Could not load existing train.csv: {e}")
        
        if test_path.exists():
            try:
                existing_test_df = pd.read_csv(test_path)
                print(f"âœ“ Found existing test data: {len(existing_test_df)} samples")
            except Exception as e:
                print(f"âš  Warning: Could not load existing test.csv: {e}")
        
        return existing_train_df, existing_test_df
    
    def _merge_datasets(self, new_train_df: pd.DataFrame, new_test_df: pd.DataFrame, 
                       existing_train_df: pd.DataFrame, existing_test_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Merge new dataset with existing dataset, removing duplicates.
        
        Args:
            new_train_df: Newly generated training data
            new_test_df: Newly generated test data
            existing_train_df: Existing training data
            existing_test_df: Existing test data
            
        Returns:
            Tuple of (merged_train_df, merged_test_df)
        """
        # Combine new and existing data
        combined_train = pd.concat([existing_train_df, new_train_df], ignore_index=True)
        combined_test = pd.concat([existing_test_df, new_test_df], ignore_index=True)
        
        # Remove exact duplicates (same text and label)
        merged_train_df = combined_train.drop_duplicates(subset=['text', 'label'], keep='first').reset_index(drop=True)
        merged_test_df = combined_test.drop_duplicates(subset=['text', 'label'], keep='first').reset_index(drop=True)
        
        print(f"ğŸ“Š Merge statistics:")
        print(f"   Training: {len(existing_train_df)} existing + {len(new_train_df)} new = {len(merged_train_df)} merged (removed {len(combined_train) - len(merged_train_df)} duplicates)")
        print(f"   Test: {len(existing_test_df)} existing + {len(new_test_df)} new = {len(merged_test_df)} merged (removed {len(combined_test) - len(merged_test_df)} duplicates)")
        
        return merged_train_df, merged_test_df
    
    def create_train_test_split(self, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Create train/test split of the dataset.
        
        Args:
            test_size: Fraction of data to use for testing
            
        Returns:
            Tuple of (train_df, test_df) DataFrames
        """
        texts, labels = self.generate_dataset()
        
        # Create DataFrame
        df = pd.DataFrame({
            'text': texts,
            'label': labels
        })
        
        # Shuffle the data
        df = df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        # Split by intent to ensure balanced representation
        train_dfs = []
        test_dfs = []
        
        for intent in self.intent_labels:
            intent_df = df[df['label'] == intent]
            split_idx = int(len(intent_df) * (1 - test_size))
            
            train_dfs.append(intent_df.iloc[:split_idx])
            test_dfs.append(intent_df.iloc[split_idx:])
        
        train_df = pd.concat(train_dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)
        test_df = pd.concat(test_dfs, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)
        
        return train_df, test_df
    
    def save_dataset(self, output_dir: str = "data", merge_existing: bool = True):
        """
        Generate and save the dataset to files.
        
        Args:
            output_dir: Directory to save the dataset files
            merge_existing: Whether to merge with existing dataset files if they exist
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # Generate train/test split for new data
        new_train_df, new_test_df = self.create_train_test_split()
        
        # Check for existing data and merge if requested
        if merge_existing:
            existing_train_df, existing_test_df = self._load_existing_dataset(output_dir)
            
            if existing_train_df is not None and existing_test_df is not None:
                print(f"ğŸ”„ Merging with existing dataset...")
                train_df, test_df = self._merge_datasets(new_train_df, new_test_df, existing_train_df, existing_test_df)
            else:
                print(f"ğŸ“ No existing dataset found, creating new dataset...")
                train_df, test_df = new_train_df, new_test_df
        else:
            print(f"ğŸ“ Creating new dataset (merge_existing=False)...")
            train_df, test_df = new_train_df, new_test_df
        
        # Save as CSV files
        train_df.to_csv(output_path / "train.csv", index=False)
        test_df.to_csv(output_path / "test.csv", index=False)
        
        # Save as JSON files for easier loading
        train_data = {
            'texts': train_df['text'].tolist(),
            'labels': train_df['label'].tolist()
        }
        test_data = {
            'texts': test_df['text'].tolist(),
            'labels': test_df['label'].tolist()
        }
        
        with open(output_path / "train.json", 'w') as f:
            json.dump(train_data, f, indent=2)
        
        with open(output_path / "test.json", 'w') as f:
            json.dump(test_data, f, indent=2)
        
        # Save label mapping
        label_mapping = {
            'intent_labels': self.intent_labels,
            'label_to_id': {label: i for i, label in enumerate(self.intent_labels)},
            'id_to_label': {i: label for i, label in enumerate(self.intent_labels)}
        }
        
        with open(output_path / "label_mapping.json", 'w') as f:
            json.dump(label_mapping, f, indent=2)
        
        print(f"Dataset saved to {output_path}")
        print(f"Training samples: {len(train_df)}")
        print(f"Test samples: {len(test_df)}")
        print(f"Intents: {len(self.intent_labels)}")
        
        # Print sample distribution
        print("\nSample distribution:")
        print(train_df['label'].value_counts().sort_index())


def main():
    """Main function to create and save the Chinese dataset."""
    # Get all available intents for help text
    all_intents = [
        "CHECK_PAYSLIP",
        "BOOK_MEETING_ROOM", 
        "REQUEST_LEAVE",
        "CHECK_BENEFITS",
        "IT_TICKET",
        "EXPENSE_REIMBURSE",
        "COMPANY_LOOKUP",
        "USER_LOOKUP"
    ]
    
    parser = argparse.ArgumentParser(description="Chinese Office Domain Intent Recognition Dataset Creator")
    parser.add_argument("--intents", type=str, nargs='+', 
                       help=f"Specific intents to generate data for. Available: {all_intents}")
    parser.add_argument("--samples-per-intent", type=int, default=100,
                       help="Number of samples to generate per intent (default: 100)")
    parser.add_argument("--output-dir", type=str, default="data",
                       help="Output directory for dataset files (default: data)")
    parser.add_argument("--no-merge", action="store_true",
                       help="Don't merge with existing dataset files")
    parser.add_argument("--interactive", action="store_true",
                       help="Interactive mode to select intents")
    
    args = parser.parse_args()
    
    print("Creating Chinese Office Domain Intent Recognition Dataset...")
    print("åˆ›å»ºä¸­æ–‡åŠå…¬é¢†åŸŸæ„å›¾è¯†åˆ«æ•°æ®é›†...")
    print()
    
    selected_intents = None
    
    if args.interactive:
        # Interactive mode
        print("Available intents:")
        for i, intent in enumerate(all_intents, 1):
            print(f"  {i}. {intent}")
        print()
        
        while True:
            choice = input("Select intents (comma-separated numbers, or 'all' for all intents): ").strip()
            if choice.lower() == 'all':
                selected_intents = None
                break
            
            try:
                indices = [int(x.strip()) - 1 for x in choice.split(',')]
                selected_intents = [all_intents[i] for i in indices if 0 <= i < len(all_intents)]
                if selected_intents:
                    break
                else:
                    print("âŒ No valid intents selected. Please try again.")
            except (ValueError, IndexError):
                print("âŒ Invalid input. Please enter comma-separated numbers or 'all'.")
    
    elif args.intents:
        selected_intents = args.intents
        # Validate selected intents
        invalid_intents = [intent for intent in selected_intents if intent not in all_intents]
        if invalid_intents:
            print(f"âŒ Error: Invalid intent(s): {invalid_intents}")
            print(f"Available intents: {all_intents}")
            return
    
    # Show what we're generating
    if selected_intents:
        print(f"ğŸ“‹ Generating data for selected intents: {selected_intents}")
    else:
        print(f"ğŸ“‹ Generating data for all intents: {all_intents}")
        
    print(f"ğŸ“Š Samples per intent: {args.samples_per_intent}")
    print(f"ğŸ“ Output directory: {args.output_dir}")
    print(f"ğŸ”„ Merge with existing: {not args.no_merge}")
    print()
    
    # Create dataset creator
    try:
        creator = ChineseOfficeIntentDatasetCreator(
            samples_per_intent=args.samples_per_intent,
            selected_intents=selected_intents
        )
    except ValueError as e:
        print(f"âŒ Error: {e}")
        return
    
    # Save dataset
    creator.save_dataset(args.output_dir, merge_existing=not args.no_merge)
    
    print("\næ•°æ®é›†åˆ›å»ºå®Œæˆï¼Dataset creation completed!")
    print("åˆ›å»ºçš„æ–‡ä»¶ Files created:")
    print(f"- {args.output_dir}/train.csv")
    print(f"- {args.output_dir}/test.csv") 
    print(f"- {args.output_dir}/train.json")
    print(f"- {args.output_dir}/test.json")
    print(f"- {args.output_dir}/label_mapping.json")


if __name__ == "__main__":
    main()